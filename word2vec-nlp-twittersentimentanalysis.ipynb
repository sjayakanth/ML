{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2510329,"sourceType":"datasetVersion","datasetId":1520310}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Natural Language Processing - Word Embedding","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport os\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom bs4 import BeautifulSoup\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nfrom nltk.downloader import download\nimport re\n\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Flatten\nfrom keras.layers import Embedding\n\nfrom wordcloud import WordCloud, STOPWORDS\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold, learning_curve\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.manifold import TSNE\nfrom sklearn.ensemble import RandomForestClassifier\nfrom gensim.models import Word2Vec","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-08-31T01:17:58.985190Z","iopub.execute_input":"2024-08-31T01:17:58.985640Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Different Approaches","metadata":{}},{"cell_type":"markdown","source":"## Training own word2vec Model","metadata":{}},{"cell_type":"code","source":"data = { 'comments': ['Social media can help people connect with others, learn, and be creative',\n         ' It can also be a way to manage social anxiety and access support',        \n         'Social media can also be a way for businesses to promote their products',\n         'Social media can lead to addiction, isolation, and poor mental health',\n         'It can also expose people to inappropriate content, cyberbullying, and privacy and data breaches',\n         'Social media can also lead to unhealthy comparisons and unrealistic body image'],\n        'class': [1,1,1,0,0,0]\n        }\ndf = pd.DataFrame(data=data , columns=[ 'comments','class'])\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\nlemm = WordNetLemmatizer()\ndef clean_data(x):\n    x_text = BeautifulSoup(x).getText()\n    x_text = x_text.lower()\n    x_text = re.sub('[^A-Za-z]',' ',x_text)\n    x_tokens = word_tokenize(x_text)\n    x_tokens = [word for word in x_tokens if word not in stop_words]\n    x_tokens = [lemm.lemmatize(word) for word in x_tokens]\n    return x_tokens","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['token'] = df['comments'].apply(clean_data)\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w2v_model = Word2Vec(df['token'],min_count=1)\nprint(w2v_model)\nprint(w2v_model.wv.key_to_index.keys()) #vocabulary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w2v_model.wv.most_similar('social')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#error\n#w2v_model.wv.most_similar('hello')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Embedding using Keras","metadata":{}},{"cell_type":"code","source":"len(set(df['comments'].str.cat(sep=' ').lower().split()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n = len(set(df['comments'].str.cat(sep=' ').lower().split()))\nencoded_ss = [one_hot(sent,n) for sent in df['comments']]\nencoded_ss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"length = len(max(encoded_ss))\npadding = pad_sequences(encoded_ss,maxlen=length,padding='pre')\npadding","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seq_model = Sequential()\nseq_model.add(Embedding(n,8,input_length=length))\nseq_model.add(Flatten())\nseq_model.add(Dense(1,activation='sigmoid'))\nseq_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seq_model.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seq_model.fit(padding,df['class'],epochs=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss, accuracy = seq_model.evaluate(padding,df['class'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = seq_model.predict(padding)\ny_pred = (predictions>0.5).astype(int)\ny_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_sentences = ['Building Relationships and connect, also to promote the products','Social media can cause sleeplessness,security breach']\n\nnew_encoded_ss = [one_hot(sent,n ) for sent in new_sentences]\nnew_encoded_ss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_padding = pad_sequences(new_encoded_ss ,maxlen=length,padding='pre')\nnew_padding","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = seq_model.predict(new_padding)\ny_pred = (predictions>0.5).astype(int)\ny_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using Pretrained Model","metadata":{}},{"cell_type":"markdown","source":"## Import Data","metadata":{}},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = ['id','entity','target','tweets']\ndata = pd.read_csv('/kaggle/input/twitter_training.csv', names=cols)\ndf = pd.DataFrame(data)\ndf.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = ['id','entity','target','tweets']\ndata = pd.read_csv('/kaggle/input/twitter_validation.csv', names=cols)\ndf_test = pd.DataFrame(data)\ndf_test.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info(),df_test.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isna().sum(),df_test.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dropna(inplace=True),df_test.dropna(inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\nword_lemm = WordNetLemmatizer()\ndef preprocess_data(x):\n    x_text = BeautifulSoup(x).getText()   \n    x_lower = x_text.lower()\n    x_spl = re.sub(\"[^a-zA-Z]\", \" \", x_lower)  \n    x_tokens = word_tokenize(x_spl)\n    x_words = [w for w in x_tokens if not w in stop_words] \n    return x_words","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_sent(data): \n    raw_sent = nltk.sent_tokenize(data.strip())        \n    sentences = [preprocess_data(sent) for sent in raw_sent if len(sent) > 0]    \n    return sentences","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence = []\nfor tweets in df['tweets']:   \n    sentence += preprocess_sent(tweets) \n    #print(preprocess_sent(tweets) )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Worker threads : Number of parallel processes to run. One thing to remember here is that unlike sklearn, it does not accept -1 option to use all the processors.\n\nDownsampling of frequent words : According to the Google documentation, values between 0.00001 and 0.001 would suffice.\n\nContext : How many words around the target word will be used?\n\nMinimum word count: This helps limit the size of the vocabulary to meaningful words. Any word that does not occur at least this many times across all documents is ignored. Reasonable values could be between 10 and 100. The reason why I chose 40 is that there are 30 reviews in each movie and repeat the title 30 times; therefore in order to avoid attaching too much importance to individual movie titles, I set to 40.","metadata":{}},{"cell_type":"code","source":"num_features = 250\nmin_count = 40\nnum_processor = 4\ncontext = 10\ndownsampling = 0.001","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# workers = num_processor, \n# vector_size = num_features, min_count = min_count,\n# window = context, sample = downsampling","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w2v_model = Word2Vec(sentence)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w2v_model.init_sims(replace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = '250features_40minwords_20context'\nw2v_model.save(model_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w2v_model.wv.most_similar('borders')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample = df['tweets'][0]\nprint('Sample1')\nprint('Original:',df['tweets'][0])\nprint('\\nProcessed Sentence:',preprocess_sent(sample))\n\nsample = df['tweets'][6]\nprint('\\nSample2')\nprint('Original:',df['tweets'][6])\nprint('\\nProcessed Sentence:',preprocess_sent(sample))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['words'] = df['tweets'].apply(preprocess_data)\ndf_test['words'] = df_test['tweets'].apply(preprocess_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getavgvec(words, model, vector_dim):\n   \n    featureVec = np.zeros((vector_dim,),dtype='float32')\n   \n    number_of_words = 0.\n    \n    index2word_set = set(model.wv.index_to_key)\n        \n    for word in words:\n        if word in index2word_set: \n            number_of_words = number_of_words + 1.\n            featureVec = np.add(featureVec,model.wv.get_vector(word))\n    \n    featureVec = np.divide(featureVec,number_of_words) #average\n    return featureVec         ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['avg_vec'] = df['words'].apply(lambda x: getavgvec(words=x,model=w2v_model,vector_dim=w2v_model.vector_size))\ndf_test['avg_vec'] = df_test['words'].apply(lambda x: getavgvec(words=x,model=w2v_model,vector_dim=w2v_model.vector_size))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"avg_feature_vec = np.array(list(df['avg_vec']))\navg_feature_vec = np.nan_to_num(avg_feature_vec)\nnp.isnan(avg_feature_vec).any()\n\ntest_avg_feature_vec = np.array(list(df_test['avg_vec']))\ntest_avg_feature_vec = np.nan_to_num(test_avg_feature_vec)\nnp.isnan(test_avg_feature_vec).any()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder = LabelEncoder()\ndf['target_encoded'] = encoder.fit_transform( df['target'])\ndf_test['target_encoded'] = encoder.transform( df_test['target'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kfold = StratifiedKFold( n_splits = 5 )\nrfc=RandomForestClassifier(random_state=42)\n\nparam_grid = { \n        'n_estimators': [200,400,800],\n    'max_features': ['sqrt', 'log2'],\n    'max_depth' : [4,8,16,32],\n    'criterion' :['gini', 'entropy']\n}\ngs_rfc = GridSearchCV(rfc, param_grid = [param_grid], verbose = 1, cv = kfold, n_jobs = -1, scoring = 'roc_auc' )\ngs_rfc.fit(avg_feature_vec, df['target_encoded'])\ngs_rfc_best = gs_rfc.best_estimator_\nprint(gs_rfc.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gs_rfc.predict(test_avg_feature_vec)\nprint(gs_rfc.best_score_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}